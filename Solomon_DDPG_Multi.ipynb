{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["YcgB7nYcMkTk","1Dv2ukFoG-dH","88TfI6wWge6O","cccav7bBNrgo","f_IBCzoSMh03","y73xqFmSMp9g","PwMNMJD7MnpE","_IJpFDzpoGqF","f8a4xY7YfPpb","TwmSYr9zkyE7","0GYy53ljlCnh","QpFybCdJlF9Z"],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyPPgcIyvuS5xkuVH8HWXyy4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Importing Libraries"],"metadata":{"id":"YcgB7nYcMkTk"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"LoY87bWDNWj_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687603599567,"user_tz":-210,"elapsed":62915,"user":{"displayName":"Narges Movahed","userId":"00752218849487334116"}},"outputId":"4d0929f8-3ff4-4dfc-cdfd-95364dfeb823"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyFgZvU4Kdx4"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import mean_squared_error\n","from matplotlib import pyplot as plt\n","\n","import pandas as pd\n","import numpy as np\n","import copy\n","import os"]},{"cell_type":"code","source":["battery_capacity = 100\n","vehicle_capacity = 200\n","vehicle_velocity = 20\n","vehicle_energy_decay = 0.2\n","energy_consumption_per_distance = 0.6\n","other_resources_consumed_per_distance = 0.2"],"metadata":{"id":"ul98d4eEpjCX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Dv2ukFoG-dH"},"source":["# DQRL Networks"]},{"cell_type":"markdown","metadata":{"id":"iYaQZkxPHQw6"},"source":["## Replay Buffer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8DMaDDFHQch"},"outputs":[],"source":["class ReplayBuffer():\n","  def __init__(self, input_shape, max_size, batch_size, action_shape):\n","    self.max_memory_buffer = max_size\n","    self.batch_size = batch_size\n","    self.memory_counter = 0\n","    self.action_shape = action_shape\n","\n","    self.state_memory = np.zeros((self.max_memory_buffer, *input_shape))\n","    self.next_state_memory = np.zeros((self.max_memory_buffer, *input_shape))\n","    self.action_memory = np.zeros((self.max_memory_buffer, *self.action_shape))\n","    self.reward_memory = np.zeros(self.max_memory_buffer)\n","    self.done_memory = np.zeros(self.max_memory_buffer, dtype=np.bool)\n","\n","  def store_transition(self, current_state, action, reward, next_state, done):\n","        index = self.memory_counter % self.max_memory_buffer\n","\n","        self.state_memory[index] = current_state\n","        self.next_state_memory[index] = next_state\n","        self.action_memory[index] = action\n","        self.reward_memory[index] = reward\n","        self.done_memory[index] = done\n","\n","        self.memory_counter += 1\n","\n","  def sample_buffer(self, batch_size = 64):\n","        replace = False\n","        if self.memory_counter < self.batch_size:\n","          replace = True\n","\n","        max_memory = min(self.memory_counter, self.max_memory_buffer)\n","        batch = np.random.choice(max_memory, self.batch_size, replace = replace)\n","\n","        states = self.state_memory[batch]\n","        actions = self.action_memory[batch]\n","        rewards = self.reward_memory[batch]\n","        next_states = self.next_state_memory[batch]\n","        dones = self.done_memory[batch]\n","\n","        return states, actions, rewards, next_states, dones\n"]},{"cell_type":"markdown","metadata":{"id":"stB4bzcsLS1g"},"source":["## Critic Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ub2UwexlLU6X"},"outputs":[],"source":["checkpoint_dir=\"/content/drive/MyDrive/Projects/Dr_Shahbazian/MDVRP/Proposed Methods/ddpg_temp\"\n","\n","class CriticNetwork(keras.Model):\n","  def __init__(self, name=\"critic\", chkpt_dir = checkpoint_dir, model_index = 0):\n","    super(CriticNetwork, self).__init__()\n","    self.model_name = name\n","    self.checkpoint_dir = chkpt_dir\n","    self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name+\"_\"+str(model_index)+'_ddpg.h5')\n","\n","    self.build_model()\n","\n","  def build_model(self):\n","    self.fc1 = Dense(512, activation='relu')\n","    self.fc2 = Dense(512, activation ='relu')\n","    self.value = Dense(1, activation=None)\n","\n","  def call(self, state, action):\n","    action_value = tf.concat([state, action], axis=-1)\n","    action_value = self.fc1(action_value)\n","    action_value = self.fc2(action_value)\n","\n","    value = self.value(action_value)\n","    return value"]},{"cell_type":"markdown","metadata":{"id":"6nI5GR3pHChI"},"source":["## Actor Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgIKnDCYHFWF"},"outputs":[],"source":["checkpoint_dir=\"/content/drive/MyDrive/Projects/Dr_Shahbazian/MDVRP/Proposed Methods/ddpg_temp\"\n","\n","class ActorNetwork(keras.Model):\n","  def __init__(self, n_actions, name=\"actor\", chkpt_dir=checkpoint_dir, model_index = 0):\n","    super(ActorNetwork, self).__init__()\n","    self.n_actions = n_actions\n","    self.model_name = name\n","    self.checkpoint_dir = chkpt_dir\n","    self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name+\"_\"+str(model_index)+\"_ddpg.h5\")\n","\n","    self.build_model()\n","\n","  def build_model(self):\n","    self.fc1=Dense(512, activation='relu')\n","    self.fc2=Dense(512, activation='relu')\n","    self.mu = Dense(self.n_actions, activation='softmax')\n","\n","  def call(self, state):\n","    prob = self.fc1(state)\n","    prob = self.fc2(prob)\n","\n","    mu = self.mu(prob)\n","    return mu"]},{"cell_type":"markdown","metadata":{"id":"H8DLQvScR64A"},"source":["## Agent Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2c0a8ajRR91b"},"outputs":[],"source":["from keras.utils.sidecar_evaluator import optimizer\n","class Agent:\n","  def __init__(self, n_actions, action_dim, input_shape, actor_lr = 0.001, critic_lr=0.002,\n","               gamma = 0.99, max_size = 2000, tau = 0.005, batch_size = 64, noise=0.1,\n","               max_action = 10, min_action = 0, exploration_rate_decay = 0.005, model_index=0):\n","    self.gamma = gamma\n","    self.tau = tau\n","    self.n_actions = n_actions\n","    self.action_dim = action_dim\n","    self.memory_buffer = ReplayBuffer(input_shape,max_size, batch_size, self.action_dim)\n","    self.noise = noise\n","    self.max_action = max_action\n","    self.min_action = min_action\n","    self.critic_lr = critic_lr\n","    self.actor_lr = actor_lr\n","    self.exploration_rate_decay = exploration_rate_decay\n","    self.model_index = model_index\n","\n","    self.actor_network = ActorNetwork(self.n_actions, name=\"actor\", model_index = self.model_index)\n","    self.target_actor = ActorNetwork(self.n_actions, name=\"target_actor\", model_index= self.model_index)\n","\n","    self.target_critic = CriticNetwork(name=\"target_critic\", model_index=self.model_index)\n","    self.critic_network = CriticNetwork(name=\"critic\", model_index=self.model_index)\n","\n","    self.actor_network.compile(loss=\"mse\", optimizer=Adam(learning_rate=actor_lr))\n","    # self.actor_network.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=actor_lr))\n","    self.target_actor.compile(optimizer=Adam(learning_rate=actor_lr))\n","\n","    self.critic_network.compile(loss= \"mse\",optimizer=Adam(learning_rate=critic_lr))\n","    self.target_critic.compile(optimizer=Adam(learning_rate=critic_lr))\n","\n","  def update_network_parameters(self,tau=None):\n","    if tau is None:\n","      return\n","    weights = []\n","    targets = []\n","    for i, weight in enumerate(self.critic.weights):\n","      weights.append(weight*tau + targets[i]*(1-tau))\n","    self.target_critic.set_weights(weights)\n","\n","  def store_memory(self, state, action, reward, next_state, done):\n","    self.memory_buffer.store_transition(state, action, reward, next_state, done)\n","\n","  def save_models(self):\n","    print(\"..... saving the model .....\")\n","    self.actor_network.save_weights(self.actor_network.checkpoint_file)\n","    self.critic_network.save_weights(self.critic_network.checkpoint_file)\n","    self.target_actor.save_weights(self.target_actor.checkpoint_file)\n","    self.target_critic.save_weights(self.target_critic.checkpoint_file)\n","\n","  def load_models(self):\n","    print(\"..... saving the model .....\")\n","    self.actor_network.save_weights(self.actor_network.checkpoint_file)\n","    self.critic_network.save_weights(self.critic_network.checkpoint_file)\n","    self.target_actor.save_weights(self.target_actor.checkpoint_file)\n","    self.target_critic.save_weights(self.target_critic.checkpoint_file)\n","\n","  def choose_action(self, observation, evaluate=False):\n","    state = tf.convert_to_tensor([observation], dtype=tf.float32)\n","    actions = self.actor_network(state)\n","    if not evaluate:\n","      actions += tf.random.normal(shape=[self.n_actions], mean=0.0, stddev=self.noise)\n","    actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n","    return actions[0]\n","\n","  def update_exploration_rate(self):\n","    self.noise = self.noise * np.exp(-self.exploration_rate_decay)\n","    print(self.noise)\n","\n","  def learn(self):\n","    # if len(self.memory_buffer.memory_buffer) < self.memory_buffer.batch_size:\n","      # return\n","    state, action, reward, next_state, done = self.memory_buffer.sample_buffer(self.memory_buffer.batch_size)\n","\n","    states = tf.convert_to_tensor(state, dtype=tf.float32)\n","    next_states = tf.convert_to_tensor(next_state, dtype=tf.float32)\n","    actions = tf.convert_to_tensor(action, dtype=tf.float32)\n","    rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n","\n","    with tf.GradientTape() as tape:\n","      target_actions = self.target_actor(next_states)\n","      temp = self.target_critic(next_states, target_actions)\n","      critic_value_next = tf.squeeze(temp,2)\n","      critic_value = tf.squeeze(self.critic_network(states, actions), 2)\n","      target = rewards + self.gamma*critic_value_next*(1-done)\n","      critic_loss = keras.losses.MSE(target, critic_value)\n","\n","    critic_network_gradient = tape.gradient(critic_loss, self.critic_network.trainable_variables)\n","    opt = tf.keras.optimizers.experimental.Adam(learning_rate=self.critic_lr)\n","    opt.apply_gradients(zip(critic_network_gradient, self.critic_network.trainable_variables))\n","\n","    with tf.GradientTape() as tape:\n","      new_policy_actions = self.actor_network(states)\n","      actor_loss = -self.critic_network(states, new_policy_actions)\n","      actor_loss = tf.math.reduce_mean(actor_loss)\n","\n","    actor_network_gradient = tape.gradient(actor_loss, self.actor_network.trainable_variables)\n","    opt = tf.keras.optimizers.experimental.Adam(learning_rate=self.actor_lr)\n","    opt.apply_gradients(zip(actor_network_gradient, self.actor_network.trainable_variables))\n","\n","    self.update_network_parameters()\n"]},{"cell_type":"markdown","source":["# Classes"],"metadata":{"id":"88TfI6wWge6O"}},{"cell_type":"markdown","source":["## Customer Class"],"metadata":{"id":"OUtYl95UKqN_"}},{"cell_type":"code","source":["class Customer:\n","    picked_up_flag = False\n","    id = 0\n","    demand = 0\n","    cx = 0\n","    cy = 0\n","    tw_start = 0\n","    tw_end = 0\n","    service_time = 0\n","\n","    def __init__(self, index, cx, cy, start_tw, end_tw, quantity, service_time):\n","        self.id = index\n","        self.cx = cx\n","        self.cy = cy\n","        self.tw_start = start_tw\n","        self.tw_end = end_tw\n","        self.demand = quantity\n","        self.service_time = service_time\n","\n","    def adapt_demand(self, new_demand):\n","        self.demand = new_demand\n","\n","    def adapt_coordinates(self, x, y):\n","        self.cx = x\n","        self.cy = y\n","\n","    def adapt_serive_time(self, new_service_time):\n","        self.service_time = new_service_time\n","\n","    def adapt_time_window(self, s_tw, e_tw):\n","        self.tw_start = s_tw\n","        self.tw_end = e_tw\n","\n","    def get_demand(self):\n","        return self.demand\n","\n","    def get_coordinates(self):\n","        return [self.cx, self.cy]\n","\n","    def get_service_time(self):\n","        return self.service_time\n","\n","    def get_time_window(self):\n","        return [self.tw_start, self.tw_end]\n","\n","    def picked_up(self):\n","        self.picked_up_flag = True\n","\n","    def get_info(self):\n","      print(\"picked_up_flag \", self.picked_up_flag,'\\n',\n","            \"id \", self.id,'\\n',\n","            \"demand \", self.demand,'\\n',\n","            \"cx \", self.cx,'\\n',\n","            \"cy \", self.cy,'\\n',\n","            \"tw_start \", self.tw_start,'\\n',\n","            \"tw_end \", self.tw_end,'\\n',\n","            \"service_time \", self.service_time,'\\n',\n","            )"],"metadata":{"id":"YDll5GlnKsTN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vehicle_class"],"metadata":{"id":"UbTGmV6VLDgn"}},{"cell_type":"code","source":["class Vehicle:\n","    vehicle_id = 0\n","    cx = 0\n","    cy = 0\n","    Max_cap = 200\n","    Max_battery = 100\n","    capacity = 100 ## Q regarding the total number of demands can be held by the vehicle\n","    current_charge = 100 ## 100 in percent\n","    departure_nodes = dict() ## Multi depots for both departure and arrival containing their ids and coordinates\n","    arrival_nodes = dict()\n","    current_departure_id = 0 ## the current departure node\n","    current_arrival_id = 0   ## the current arrival node\n","    max_travel_time = 1000\n","    current_travel_time = 0\n","    energy_decay = 0.02\n","    velocity = 1\n","\n","    def __init__(self):\n","        return\n","\n","    def initiate(self, index, x, y, dep_nodes, arr_nodes, cap_max, max_T, energy_decay, battery_total_capacity, velocity):\n","        self.vehicle_id = index\n","        self.cx = x\n","        self.cy = y\n","        self.departure_nodes = dep_nodes\n","        self.arrival_nodes = arr_nodes\n","        self.capacity = cap_max\n","        self.max_cap = cap_max\n","        self.max_travel_time = max_T\n","        self.current_travel_time = 0\n","        self.current_charge = battery_total_capacity\n","        self.max_battery = battery_total_capacity\n","        self.energy_decay_per_distance = energy_decay\n","        if len(self.departure_nodes) > 1:\n","          depot_keys = list(self.departure_nodes.keys()) ## random selection of departure nodes\n","          rand_index = depot_keys[np.random.randint(0,len(depot_keys)-1)]\n","          self.current_departure_id = rand_index\n","\n","          depot_keys = list(self.arrival_nodes.keys()) ## random selection of arrival nodes\n","          rand_index = depot_keys[np.random.randint(0,len(depot_keys)-1)]\n","          self.current_arrival_id = rand_index\n","        else:\n","          dep_key = list(self.departure_nodes.keys())[0]\n","          arr_key = list(self.arrival_nodes.keys())[0]\n","          self.current_departure_id = dep_key\n","          self.current_arrival_id = arr_key\n","\n","    def set_current_depot_ids(self, departure, arrival, random = False):\n","\n","        if len(self.departure_nodes) > 1:\n","          if random:\n","            depot_keys = list(self.departure_nodes.keys()) ## random selection of departure nodes\n","            rand_index = depot_keys[np.random.randint(0,len(depot_keys)-1)]\n","            self.current_departure_id = rand_index\n","\n","            depot_keys = list(self.arrival_nodes.keys()) ## random selection of arrival nodes\n","            rand_index = depot_keys[np.random.randint(0,len(depot_keys)-1)]\n","            self.current_arrival_id = rand_index\n","          else:\n","            self.current_departure_id = departure\n","            self.current_arrival_id = arrival\n","        else: ## there is only one option\n","          dep_key = list(self.departure_nodes.keys())[0]\n","          arr_key = list(self.arrival_nodes.keys())[0]\n","          self.current_departure_id = dep_key\n","          self.current_arrival_id = arr_key\n","\n","\n","    def get_coordinates(self):\n","        return [self.cx, self.cy]\n","\n","    def get_info(self):\n","          print(\"id \", self.vehicle_id,'\\n',\n","                \"cx \", self.cx, '\\n',\n","                \"cy \", self.cy,'\\n',\n","                \"Max_cap \", self.Max_cap,'\\n',\n","                \"Max_battery \", self.Max_battery,'\\n',\n","                \"capacity \", self.capacity,'\\n',\n","                \"current_charge \", self.current_charge,'\\n',\n","                \"departure_nodes \", self.departure_nodes,'\\n',\n","                \"arrival_nodes \", self.arrival_nodes,'\\n',\n","                \"current_departure_id \", self.current_departure_id,'\\n',\n","                \"current_arrival_id \", self.current_arrival_id,'\\n',\n","                \"max_travel_time \", self.max_travel_time,'\\n',\n","                \"current_travel_time \", self.current_travel_time,'\\n',\n","                \"energy_decay \", self.energy_decay\n","                )"],"metadata":{"id":"0y_vH5GgLFR3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Environment Classes"],"metadata":{"id":"g5nsG5OULWUF"}},{"cell_type":"markdown","source":["### State and Action Classes"],"metadata":{"id":"7hiJ0OwFWPza"}},{"cell_type":"code","source":["class StateClass: ## each state indicates the whole current setting of the environment\n","    time_step = 0\n","    location_id = 0 ## current client or depot id => the node id on which the vehicle currently is\n","    vehicle_location = tuple() # for one vehicle\n","    vehicle_charge = float()\n","    clients_locations = dict() # the current state of all the clients\n","    clients_demands = dict() # the current modified state of demands of all the clients\n","\n","    def get_vector(self): ## get the vector of the vehicle and all the clients for a the current time step\n","        vectors = dict()\n","        for client in self.clients_locations.keys(): ## vectors of the current state of each client\n","            state = np.zeros(shape=(1, 6))[0]\n","            state[0], state[1] = self.vehicle_location[0], self.vehicle_location[1]\n","            state[2] = self.vehicle_charge\n","            state[3], state[4] = self.clients_locations[client][0], self.clients_locations[client][1]\n","            state[5] = self.clients_demands[client]\n","            vectors[client] = state\n","        return vectors\n","\n","    def client_vector(self, client_id):\n","            state = np.zeros(shape=(1, 6))[0]\n","            state[0], state[1] = self.vehicle_location[0], self.vehicle_location[1]\n","            state[2] = self.vehicle_charge\n","            state[3], state[4] = self.clients_locations[client_id][0], self.clients_locations[client_id][1]\n","            state[5] = self.clients_demands[client_id]\n","            return state\n","\n","    def vehicle_location_modifier(self, x, y):\n","        self.vehicle_location = (x,y)\n","\n","    def demand_modifier(self, client_id, new_demand):\n","        self.clients_demands[client_id] = new_demand\n","\n","    def get_info(self):\n","      print(\"time_step \", self.time_step,'\\n',\n","            \"location_id \", self.location_id,'\\n',\n","            \"vehicle_location \", self.vehicle_location,'\\n',\n","            \"clients_locations \", self.clients_locations,'\\n',\n","            \"clients_demands \", self.clients_demands,'\\n',\n","            )"],"metadata":{"id":"x6k5IBXLPsEp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ActionClass:\n","    next_customer_id = 0\n","    next_customer_location = tuple()\n","    vehicle_id = 0\n","    depot_id = 0\n","    vehicle_speed = 0\n","    vehicle_acceleration = 0\n","\n","    def get_vector(self): ## get the vector of the vehicle and all the clients for a the current time step\n","        action_vec = np.zeros(shape=(1, 6))[0]\n","        action_vec[0] = self.next_customer_id\n","        action_vec[1], action_vec[1] = self.next_customer_location[0], self.next_customer_location[1]\n","        action_vec[3] = self.vehicle_id\n","        action_vec[4] = self.depot_id\n","        action_vec[5] = self.vehicle_speed\n","        return action_vec"],"metadata":{"id":"79hZodT0PuBZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Depot vector generator"],"metadata":{"id":"cccav7bBNrgo"}},{"cell_type":"code","source":["def depot_vectors(depots, vehicle, depot_id):\n","      state = np.zeros(shape=(1, 6))[0]\n","      state[0], state[1] = vehicle.cx, vehicle.cy\n","      state[2] = vehicle.current_charge\n","      state[3], state[4] = depots[depot_id]['dep_x'], depots[depot_id]['dep_y']\n","      state[5] = 0\n","      return state\n"],"metadata":{"id":"7myJVxLrNknb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Environment"],"metadata":{"id":"FCZybkZn66Ss"}},{"cell_type":"code","source":["time_travelled_per_distance = 1\n","energy_consumption_per_distance = 1\n","other_resources_consumed_per_distance = 1"],"metadata":{"id":"LPV_Nevlv6qZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from scipy.spatial.distance import euclidean\n","\n","class VRP_environment:\n","    ## taken states and actions (the experienced settings of the environment)\n","    states_list = list()\n","    actions_list = list()\n","    current_route = list()\n","    routes = list()\n","\n","    ## flags\n","    idle = False ## indicating being\n","    full_capacity = 0 ## indicating the vehicle's capacity is filled\n","    done = False ## indicating the stop condition of the environment (all clients have been seen or the time step restriction has passed)\n","\n","    ## the possible settings\n","    clients = dict()\n","    depots = dict()\n","\n","    ## the current vehicle and state configurations\n","    vehicle = Vehicle() ## the vehicle associated with this environment and its current configurations\n","    current_state = StateClass()\n","    depot_shift_index = len(clients)\n","\n","    def __init__(self, clients, vehicle, vehicle_id, depots):\n","        self.clients = clients ## a dictionary of all the clients and their datas\n","        self.vehicle = vehicle ## variable of the vehicle class containing all the required data about the vehicle\n","        self.depots = depots\n","\n","        initial_state = StateClass()\n","        initial_state.time_step = 0\n","        initial_state.location_id = self.vehicle.current_departure_id\n","        initial_state.vehicle_location = (self.vehicle.cx, self.vehicle.cy)\n","        initial_state.vehicle_charge = self.vehicle.current_charge\n","\n","        client_keys = list(self.clients.keys())\n","        for client in client_keys:\n","            initial_state.clients_locations[client] = self.clients[client].get_coordinates()\n","            initial_state.clients_demands[client] = self.clients[client].get_demand()\n","\n","        current_route = list()\n","        routes = list()\n","        for client in self.clients:\n","            self.clients[client].picked_up_flag = False\n","\n","        self.states_list.append(initial_state)\n","        self.current_state = initial_state\n","\n","        self.current_route.append(initial_state.location_id)\n","\n","    def take_action(self, time_step, next_client_id):\n","        next_client = self.clients[next_client_id]\n","\n","       ## defining the action\n","        action = ActionClass()\n","        action.next_customer_id = next_client_id\n","        action.next_customer_location = (next_client.cx, next_client.cy)\n","        action.vehicle_id = self.vehicle.vehicle_id\n","        action.vehicle_speed = 0\n","        action.vehicle_acceleration = 0\n","\n","        ## taking the action\n","        next_state = StateClass()\n","        next_state.time_step = time_step\n","        next_state.location_id = next_client_id\n","        next_state.vehicle_location = (next_client.cx, next_client.cy)\n","        next_state.vehicle_charge = self.vehicle.current_charge - 2     ## Revise: modify this for the energy consumed by taking the action\n","        next_state.client_id = next_client_id\n","        next_state.clients_locations = self.current_state.clients_locations\n","        next_state.clients_demands = self.current_state.clients_demands\n","\n","        self.vehicle.cx = next_client.cx\n","        self.vehicle.cy = next_client.cy\n","        self.vehicle.current_charge = next_state.vehicle_charge\n","\n","        # print(\"vehicle capacity before taking the action\", self.vehicle.capacity)\n","        new_demand = next_state.clients_demands[next_client_id] - self.vehicle.capacity\n","        self.vehicle.capacity = self.vehicle.capacity - next_state.clients_demands[next_client_id]\n","        if new_demand <=0:\n","            new_demand = 0\n","\n","        # print(\"new vehicle capacity=\", self.vehicle.capacity, \"new demand=\", new_demand,\n","        #       \"previous demand=\", next_state.clients_demands[next_client_id])\n","\n","        next_state.clients_demands[next_client_id] = new_demand\n","\n","        self.clients[next_client_id].picked_up()\n","        self.states_list.append(next_state)\n","\n","        if self.vehicle.capacity <= 0:\n","            self.full_capacity = True\n","\n","        return action, next_state\n","\n","    def return_to_depot(self, timestep):\n","        depot_id = self.vehicle.current_arrival_id\n","\n","        ## defining the depot state\n","        depot_state = StateClass()\n","        depot_state.time_step = timestep\n","        depot_state.client_id = depot_id\n","        depot_state.vehicle_location = (float(self.depots[depot_id]['dep_x']), float(self.depots[depot_id]['dep_y']))\n","        depot_state.vehicle_charge = self.vehicle.current_charge\n","        depot_state.clients_locations = self.current_state.clients_locations ## the locations and demands do not change\n","        depot_state.clients_demands = self.current_state.clients_demands\n","\n","        ## defining the action for the depot\n","        action = ActionClass()\n","        action.next_customer_id = depot_id\n","        action.next_customer_location = (self.depots[depot_id]['dep_x'], self.depots[depot_id]['dep_y'])\n","        action.vehicle_id = self.vehicle.vehicle_id\n","        action.vehicle_speed = 0\n","        action.vehicle_acceleration = 0\n","\n","        self.states_list.append(depot_state)\n","\n","        self.vehicle.current_charge = self.vehicle.Max_battery\n","        self.vehicle.cx = self.depots[depot_id]['dep_x']\n","        self.vehicle.cy = self.depots[depot_id]['dep_y']\n","        self.vehicle.capacity = self.vehicle.max_cap\n","        self.vehicle.current_charge = self.vehicle.max_battery\n","\n","        self.full_capacity = False\n","\n","        return action, depot_state\n","\n","\n","    def remaining_state_check(self):\n","      for client in self.clients.keys():\n","        if not self.clients[client].picked_up_flag:\n","          return True\n","      return False\n","\n","    def terminal_check(self):\n","      # print(\"all clients seen\", not self.remaining_state_check(), self.vehicle.vehicle_id)\n","      return (not self.remaining_state_check())\n","\n","    def reward_function_3(self, action, tw_violated, reached_depot):\n","      reward = 0\n","      pickedup_users = list()\n","      notpicked_up = list()\n","      for c in self.clients.keys():\n","          if self.clients[c].picked_up_flag == 1:\n","              pickedup_users.append(c)\n","          elif self.clients[c].picked_up_flag == 0:\n","              notpicked_up.append(c)\n","\n","      if tw_violated:\n","          # print(\"first_option: time window violated\")\n","          demands = 0\n","          for client in pickedup_users:\n","              demands += self.clients[client].demand * np.abs(self.clients[client].tw_end - self.clients[client].tw_start)\n","          reward = -demands\n","\n","      elif self.idle or tw_violated:\n","          # print(\"second_option: idle or time_window violated\")\n","          reward = -1000\n","\n","      elif reached_depot and len(notpicked_up)==0 :\n","          # print(\"third_option: reached depot with no user\")\n","          reward = +1000\n","\n","      elif reached_depot and len(notpicked_up)>0:\n","          # print(\"forth_option: reached depot with at least one user\")\n","          demands = 0\n","          for client in pickedup_users:\n","              demands += self.clients[client].demand * 100\n","          t_p1p2 = 10 # TODO: determine the time from the starting depot to the ending\n","          reward = demands - 10*t_p1p2\n","\n","      else:\n","          # print(\"fifth_option\")\n","          demands = 0\n","          for client in pickedup_users:\n","              demands += self.clients[client].demand**2\n","          t_p1p2 = 10 # TODO: determine the time from the starting depot to the ending\n","          reward = - 10*t_p1p2 - demands\n","\n","      return reward"],"metadata":{"id":"djKBQSP5LaGa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Reading the Dataset"],"metadata":{"id":"f_IBCzoSMh03"}},{"cell_type":"code","source":["dataset_num = 11\n","dataset = \"R\"\n","\n","# vehicle_num = {25:4 ,\n","#                50:8 ,\n","#                100:15}"],"metadata":{"id":"9lumgwi4oxCf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Functions"],"metadata":{"id":"y73xqFmSMp9g"}},{"cell_type":"code","source":["def client_data_structure(nodes_data):\n","    # max_quantity = float(max(time_windows['request|quantity']))\n","\n","    customers = dict()\n","    customer_id = 0\n","    for row in range(len(nodes_data)):\n","        id, cx, cy = nodes_data['CUST_NO.'][row], float(nodes_data['XCOORD.'][row]), float(nodes_data['YCOORD.'][row])\n","    #     # cx /= 100\n","    #     # cy/= 100\n","        tw, quantity, service_time = (nodes_data['READY_TIME'][row], nodes_data['DUE_DATE'][row]),\\\n","                                      float(nodes_data['DEMAND'][row]),\\\n","                                      float(nodes_data['SERVICE_TIME'][row])\n","\n","    #     # quantity /= max_quantity\n","    #     # service_time /= 100\n","\n","        customers[customer_id] = Customer(customer_id, float(cx), float(cy), float(tw[0]), float(tw[1]),\n","                                          float(quantity), float(service_time))\n","        customer_id += 1\n","\n","    return customers"],"metadata":{"id":"0w5T452NMrQA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["battery_capacity = 100\n","\n","def vehicle_depot_data_structure(nodes_data, depot_shift_index):\n","\n","    vehicles = dict()\n","    depots = dict()\n","\n","    vehicle_id = 0\n","    depot_id = depot_shift_index\n","\n","    for row in range(len(nodes_data)):\n","        ind, cx, cy = nodes_data['CUST_NO.'][row], float(nodes_data['XCOORD.'][row]), float(nodes_data['YCOORD.'][row])\n","        # cx /= 100\n","        # cy/= 100\n","\n","        capacity = 200\n","        max_travel_time = nodes_data['DUE_DATE'][row]\n","\n","        depots[depot_id] = {'dep_x':cx, 'dep_y':cy}\n","\n","        vehicles[vehicle_id] = Vehicle()\n","        vehicles[vehicle_id].initiate(vehicle_id, float(cx), float(cy), depots, depots,\n","                                    float(capacity), float(max_travel_time), 0.02, battery_capacity, vehicle_velocity)\n","        vehicle_id += 1\n","        depot_id += 1\n","\n","    return vehicles, depots"],"metadata":{"id":"P282S17VSN7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def creat_data_model(train_df, valid_df, test_df):\n","  depot_num_train = 1\n","  train_depots = train_df[0:depot_num_train]\n","  train_data = train_df[depot_num_train:]\n","  train_data = train_data.reset_index(drop=True)\n","\n","  depot_num_valid = 1\n","  valid_depots = valid_df[0:depot_num_valid]\n","  valid_data = valid_df[depot_num_valid:]\n","  valid_data = valid_data.reset_index(drop=True)\n","\n","  depot_num_test = 1\n","  test_depots = test_df[0:depot_num_test]\n","  test_data = test_df[depot_num_test:]\n","  test_data = test_data.reset_index(drop=True)\n","\n","  train_clients_data = client_data_structure(train_data)\n","  valid_clients_data = client_data_structure(valid_data)\n","  test_clients_data = client_data_structure(test_data)\n","\n","  train_depot_shift = len(train_clients_data)\n","  valid_depot_shift = len(valid_clients_data)\n","  test_depot_shift = len(test_clients_data)\n","\n","  train_vehicles, train_depots = vehicle_depot_data_structure(train_depots, train_depot_shift)\n","  valid_vehicles, valid_depots = vehicle_depot_data_structure(valid_depots, valid_depot_shift)\n","  test_vehicles, test_depots = vehicle_depot_data_structure(test_depots, test_depot_shift)\n","\n","  train = {'clients': train_clients_data,\n","              'depots': train_depots,\n","              'vehicles': train_vehicles}\n","\n","  valid = {'clients': valid_clients_data,\n","                'depots': valid_depots,\n","                'vehicles': valid_vehicles}\n","\n","  test = {'clients': test_clients_data,\n","                'depots': test_depots,\n","                'vehicles': test_vehicles}\n","\n","  return train, valid, test"],"metadata":{"id":"Rxg6HbBXipEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_data_ind(dataframe, depot_num):\n","  depots = dataframe[0:depot_num]\n","  data = dataframe[depot_num:]\n","  data = data.reset_index(drop=True)\n","\n","  clients_data = client_data_structure(data)\n","\n","  depot_shift = len(clients_data)\n","\n","  data_vehicles, data_depots = vehicle_depot_data_structure(depots, depot_shift)\n","\n","  data = {'clients': clients_data,\n","              'depots': data_depots,\n","              'vehicles': data_vehicles}\n","\n","  return data"],"metadata":{"id":"MrrYbZcqjVKf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preparation"],"metadata":{"id":"PwMNMJD7MnpE"}},{"cell_type":"code","source":["## train_data\n","# data_path = \"/content/drive/MyDrive/Projects/Dr_Shahbazian/MDVRP/Datasets/Solomon/R1/r101.txt\"\n","\n","if dataset_num <=9:\n","  if dataset == \"C\":\n","    data_path = \"/content/drive/MyDrive/Projects/Dr_Shahbazian/MDVRP/Datasets/Solomon/C1/c10\"+str(dataset_num)+\".txt\"\n","  elif dataset == \"RC\":\n","    data_path = \"/content/drive/MyDrive/Projects/Dr_Shahbazian/MDVRP/Datasets/Solomon/RC1/rc10\"+str(dataset_num)+\".txt\"\n","  elif dataset == \"R\":\n","    data_path = \"/content/drive/MyDrive/Projects/Dr_Shahbazian/MDVRP/Datasets/Solomon/R1/r10\"+str(dataset_num)+\".txt\"\n","else:\n","  if dataset == \"C\":\n","    data_path = \"/content/drive/MyDrive/Projects/Dr_Shahbazian/MDVRP/Datasets/Solomon/C1/c1\"+str(dataset_num)+\".txt\"\n","  elif dataset == \"RC\":\n","    data_path = \"/content/drive/MyDrive/Projects/Dr_Shahbazian/MDVRP/Datasets/Solomon/RC1/rc1\"+str(dataset_num)+\".txt\"\n","  elif dataset == \"R\":\n","    data_path = \"/content/drive/MyDrive/Projects/Dr_Shahbazian/MDVRP/Datasets/Solomon/R1/r1\"+str(dataset_num)+\".txt\"\n","\n","data_df = pd.read_csv(data_path, delim_whitespace=True)\n","\n","depot_num = 1\n","depots = data_df[0:depot_num]\n","vehicle_nodes = depots\n","data_df = data_df[depot_num:]\n","\n","train_df = depots\n","valid_df = depots\n","test_df = depots\n","\n","train_test_ratio = 0.25\n","train_valid_ratio = 0.5\n","train_test_length = int(len(data_df)*train_test_ratio)\n","train_valid_length = int(len(data_df)*train_valid_ratio)\n","print(train_valid_length)\n","print(train_test_length)\n","\n","train_nodes = data_df[0:]\n","train_df = pd.concat([train_df,train_nodes], ignore_index=True)\n","\n","valid_nodes = data_df[train_valid_length:train_valid_length+train_test_length]\n","valid_df = pd.concat([valid_df,valid_nodes], ignore_index=True)\n","\n","test_nodes = data_df[train_valid_length+train_test_length:]\n","test_df = pd.concat([test_df,test_nodes], ignore_index=True)\n","\n","train_df = train_df.reset_index(drop=True)\n","valid_df = valid_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","print(\"train\",len(train_df))\n","print(\"valid\",len(valid_df))\n","print(\"test\",len(test_df))\n","\n","train_data = create_data_ind(train_df, depot_num)\n","valid_data = create_data_ind(valid_df, depot_num)\n","test_data = create_data_ind(test_df, depot_num)"],"metadata":{"id":"9s2g4PF0Kcby","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687603605949,"user_tz":-210,"elapsed":2751,"user":{"displayName":"Narges Movahed","userId":"00752218849487334116"}},"outputId":"21ce95d7-057d-43eb-c75f-2306fa3aed99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["50\n","25\n","train 101\n","valid 26\n","test 26\n"]}]},{"cell_type":"markdown","source":["# Algorithms"],"metadata":{"id":"JPqdXOV68SMy"}},{"cell_type":"markdown","source":["## Multi Vehicle"],"metadata":{"id":"6ClLKEVrjl8N"}},{"cell_type":"markdown","source":["### Adding additional depots and vehicles"],"metadata":{"id":"_IJpFDzpoGqF"}},{"cell_type":"code","source":["def adding_depot_vehicles(data, v_num):\n","  ## modifying the problem to multi-depot and multi_vehicle ## ToDo: remove for the real dataset\n","  depot = {'dep_x':'30.0', 'dep_y': '60.0'}\n","  data['depots'][max(data['depots'].keys())+1] = depot\n","\n","  id = max(data['vehicles'].keys())\n","  added_vehicles_data = {}\n","  for i in range(v_num):\n","    id += 1\n","    added_vehicles_data[i] = Vehicle()\n","    added_vehicles_data[i].initiate(id+1, 30.0, 40.0, data['depots'], data['depots'], vehicle_capacity, 1000, vehicle_energy_decay, battery_capacity, vehicle_velocity)\n","    data['vehicles'][id] = added_vehicles_data[i]\n","\n","  return data"],"metadata":{"id":"aqvPgJefdU68"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Validation Multi Vehicle"],"metadata":{"id":"f8a4xY7YfPpb"}},{"cell_type":"code","source":["def validate_multi_vehicle(valid_df,  v_num, vehicle_id, agent):\n","  print(\"Validation ...\")\n","\n","  valid_data = create_data_ind(valid_df, 1)\n","  valid_data = adding_depot_vehicles(valid_data, v_num-1)\n","  clients = valid_data['clients']\n","  vehicles = valid_data['vehicles']\n","  depots = valid_data['depots']\n","\n","  state_dim = 6\n","  action_dim = (1,len(clients))\n","\n","  episode_reward = list()\n","  total_routes = dict()\n","\n","  evaluate = False\n","  exploration_rate = 0.4\n","  load_checkpoint = False\n","  best_score = -np.inf\n","  avg_score = 0\n","\n","  env = VRP_environment(clients, vehicles[vehicle_id], vehicle_id, depots)\n","  env.routes = []\n","  env.current_route = [env.vehicle.current_departure_id]\n","\n","  env.done = False\n","  rewards = []\n","  time_step = 0\n","  total_steps = 0\n","  while not env.done:\n","      demands = set(env.current_state.clients_demands.values())\n","      if 0 in demands:\n","        demands.remove(0)\n","      if env.full_capacity or env.vehicle.capacity < min(demands): ##ToDO: recheck for the random or deliberate selection of the depot to go\n","          selected_action, next_state = env.return_to_depot(time_step)\n","          reward = env.reward_function_3(selected_action, False, True)\n","          next_client_id = env.vehicle.current_arrival_id\n","          next_state_vector = depot_vectors(depots, env.vehicle, env.vehicle.current_arrival_id)\n","          action_id = env.vehicle.current_arrival_id\n","          env.current_route.append(next_client_id)\n","          env.routes.append(env.current_route)\n","          env.current_route = [next_client_id]\n","\n","      else:\n","        current_state_id = env.current_state.location_id\n","        if current_state_id in depots.keys(): ## the current state is the depot\n","            current_state_vector = depot_vectors(depots, env.vehicle, current_state_id)\n","        elif current_state_id in clients.keys(): ## the current state is one of the clients\n","            current_state_vector = env.current_state.client_vector(current_state_id)\n","\n","        current_state_vector = current_state_vector.reshape(1,6)\n","\n","        env.idle = False\n","        reward = 0\n","\n","        actions = agent.choose_action(current_state_vector, evaluate)\n","        q_values = np.array(actions[0])\n","        q_values = q_values[0:len(clients)]\n","        next_client_id = np.argmax(q_values)\n","        repeat = 0\n","        while next_client_id not in list(env.clients.keys()) and env.remaining_state_check() and repeat <= len(env.clients):\n","            q_values[next_client_id] = -np.inf\n","            next_client_id = np.argmax(q_values)\n","            repeat += 1\n","            if next_client_id in list(env.clients.keys()) and not env.clients[next_client_id].picked_up_flag:\n","              break\n","\n","        next_client = env.clients[next_client_id]\n","\n","        if env.vehicle.capacity < next_client.demand or next_client.picked_up_flag == True:\n","          continue\n","\n","        if next_client.tw_start <=time_step <= next_client.tw_end: ## checking the time window perservation constraint\n","          selected_action, next_state = env.take_action(time_step, next_client_id)\n","          reward = env.reward_function_3(selected_action, True, False)\n","\n","        elif time_step < next_client.tw_start: ## the vehicle needs to wait for the time step in which the client will come\n","            env.idle = True\n","            env.vehicle.current_charge *= (1-env.vehicle.energy_decay) * np.abs(next_client.tw_start - time_step) ## charge decay due to being idle\n","            time_step = next_client.tw_start ##shifting the time step to avoid wasting time\n","            selected_action, next_state = env.take_action(time_step, next_client_id)\n","            reward = env.reward_function_3(selected_action, True, False)\n","\n","        elif time_step > next_client.tw_end:\n","            ## the action is taken and the client is also picked up but the vehicle receives a penalty\n","            selected_action, next_state = env.take_action(time_step, next_client_id)\n","            reward = env.reward_function_3(selected_action, True, False)\n","\n","        next_state_vector = next_state.client_vector(next_client_id)\n","\n","        agent.store_memory(current_state_vector, actions, reward, np.array([next_state_vector]), int(env.done))\n","        rewards.append(reward)\n","\n","        env.current_state = next_state\n","        env.current_route.append(next_client_id)\n","\n","      env.done = env.terminal_check()\n","      if env.done:\n","        agent.update_exploration_rate()\n","        if env.current_route[-1] not in depots.keys():\n","          selected_action, next_state = env.return_to_depot(time_step)\n","          reward = env.reward_function_3(selected_action, False, True)\n","          next_client_id = env.vehicle.current_arrival_id\n","          next_state_vector = depot_vectors(depots, env.vehicle, env.vehicle.current_arrival_id)\n","          action_id = env.vehicle.current_arrival_id\n","          env.current_route.append(next_client_id)\n","          rewards.append(reward)\n","\n","        env.routes.append(env.current_route)\n","        env.current_route = []\n","\n","      time_step += 1\n","      total_steps += 1\n","\n","  agent.learn()\n","\n","  return np.mean(rewards), env.routes, agent"],"metadata":{"id":"E6CX3XTcfRoQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train Multi Vehicle"],"metadata":{"id":"TwmSYr9zkyE7"}},{"cell_type":"code","source":["def train_multi_vehicle(n_episodes, n_time_steps, train_df, valid_df, n_vehicles, agent_update_frequency = 10):\n","\n","  train_data = create_data_ind(train_df, 1)\n","  train_data = adding_depot_vehicles(train_data, n_vehicles-1)\n","  clients = train_data['clients']\n","  vehicles = train_data['vehicles']\n","  depots = train_data['depots']\n","\n","  state_dim = 6\n","  action_dim = (1,len(clients))\n","\n","  evaluate = False\n","  exploration_rate = 0.4\n","\n","  load_checkpoint = False\n","  best_score = dict()\n","  avg_score = dict()\n","  score_history = dict()\n","\n","  envs = dict() ## one environment for each vehicle\n","  total_vehicle_routes = dict() ## total passed routes for each vehicle based on each episode\n","  total_episode_route = dict() ## total passed routes for all the episodes (inside of it is the above dictionary)\n","  vehicle_step_acumulative_reward = dict() ## acumulative rewards for each vehicle at the end of each episode\n","  agents = dict()\n","  Target_nets = dict()\n","\n","  mean_valid = {}\n","  valid_routes = {}\n","  best_rewards = {}\n","\n","  for vehicle in vehicles.keys():\n","      agents[vehicle] = Agent(len(clients), action_dim, (1,state_dim), noise = exploration_rate, model_index=vehicle)\n","      total_vehicle_routes[vehicle] = dict()\n","      best_score[vehicle] = -np.inf\n","      avg_score[vehicle] = 0\n","      score_history[vehicle] = []\n","      best_rewards[vehicle] = -np.inf\n","\n","  for episode in range(n_episodes):\n","      # print(\"\\n episode = \", episode)\n","      global_time_step = 0\n","\n","      train_data = create_data_ind(train_df, 1)\n","      train_data = adding_depot_vehicles(train_data, n_vehicles-1)\n","      clients = train_data['clients']\n","      vehicles = train_data['vehicles']\n","      depots = train_data['depots']\n","\n","      for vehicle_id in vehicles.keys():\n","          vehicle_step_acumulative_reward[(episode, vehicle_id)] = 0\n","          envs[vehicle_id] = VRP_environment(clients, vehicles[vehicle_id], vehicle_id, depots)\n","          envs[vehicle_id].routes = []\n","          envs[vehicle_id].current_route = [envs[vehicle_id].vehicle.current_departure_id]\n","          envs[vehicle_id].done = False\n","          envs[vehicle_id].full_capacity = False\n","          envs[vehicle_id].vehicle.current_travel_time = 0\n","\n","      while global_time_step <= n_time_steps:\n","        for v_id in range(n_vehicles):\n","          if global_time_step < envs[v_id].vehicle.current_travel_time and envs[v_id].idle:\n","                reward = -1000\n","                vehicle_step_acumulative_reward[(episode, v_id)] += reward\n","\n","          elif global_time_step >= envs[v_id].vehicle.current_travel_time and not envs[v_id].done:\n","              demands = set(envs[v_id].current_state.clients_demands.values())\n","              min_demand = 0\n","              if 0 in demands:\n","                demands.remove(0)\n","              if len(demands)>0:\n","                min_demand =  min(demands)\n","              if envs[v_id].full_capacity or envs[v_id].vehicle.capacity < min_demand: ##ToDO: recheck for the random or deliberate selection of the depot to go\n","                  selected_action, next_state = envs[v_id].return_to_depot(envs[v_id].vehicle.current_travel_time)\n","                  reward = envs[v_id].reward_function_3(selected_action, False, True)\n","                  next_client_id = envs[v_id].vehicle.current_arrival_id\n","                  next_state_vector = depot_vectors(envs[v_id].depots, envs[v_id].vehicle, envs[v_id].vehicle.current_arrival_id)\n","                  action_id = envs[v_id].vehicle.current_arrival_id\n","\n","                  envs[v_id].current_route.append(next_client_id)\n","                  envs[v_id].routes.append(envs[v_id].current_route)\n","                  envs[v_id].current_route = []\n","\n","              elif envs[v_id].remaining_state_check():\n","                  current_state_id = envs[v_id].current_state.location_id\n","                  if current_state_id in envs[v_id].depots.keys(): ## the current state is the depot\n","                      current_state_vector = depot_vectors(envs[v_id].depots, envs[v_id].vehicle, current_state_id)\n","                  elif current_state_id in envs[v_id].clients.keys(): ## the current state is one of the clients\n","                      current_state_vector = envs[v_id].current_state.client_vector(current_state_id)\n","                      current_state_vector = current_state_vector.reshape(1,6)\n","\n","                  envs[v_id].idle = False\n","\n","                  actions = agents[v_id].choose_action(current_state_vector, evaluate)\n","                  q_values = np.array(actions)\n","                  q_values = q_values.reshape((1, 100))\n","                  next_client_id = np.argmax(q_values)\n","                  next_client = envs[v_id].clients[next_client_id]\n","\n","                  repeat = 0\n","                  while next_client.picked_up_flag and envs[v_id].remaining_state_check() and repeat <= len(envs[v_id].clients):\n","                      q_values[0][next_client_id] = -np.inf\n","                      next_client_id = np.argmax(q_values)\n","                      next_client = envs[v_id].clients[next_client_id]\n","                      repeat += 1\n","\n","                  next_client = envs[v_id].clients[next_client_id]\n","\n","                  if envs[v_id].vehicle.capacity < next_client.demand or next_client.picked_up_flag == True:\n","                    continue\n","\n","                  if next_client.tw_start <= envs[v_id].vehicle.current_travel_time <= next_client.tw_end: ## checking the time window perservation constraint\n","                    selected_action, next_state = envs[v_id].take_action(envs[v_id].vehicle.current_travel_time, next_client_id)\n","                    reward = envs[v_id].reward_function_3(selected_action, False, False)\n","                    envs[v_id].clients[next_state.location_id].picked_up()\n","\n","                  elif envs[v_id].vehicle.current_travel_time < next_client.tw_start: ## the vehicle needs to wait for the time step in which the client will come\n","                      envs[v_id].idle = True\n","                      envs[v_id].vehicle.current_charge *= (1-envs[v_id].vehicle.energy_decay) * np.abs(next_client.tw_start - envs[v_id].vehicle.current_travel_time) ## charge decay due to being idle\n","                      envs[v_id].vehicle.current_travel_time = next_client.tw_start ##shifting the time step to avoid wasting time\n","                      selected_action, next_state = envs[v_id].take_action(envs[v_id].vehicle.current_travel_time, next_client_id)\n","                      reward = envs[v_id].reward_function_3(selected_action, True, False)\n","                      envs[v_id].clients[next_state.location_id].picked_up()\n","\n","                  elif envs[v_id].vehicle.current_travel_time > next_client.tw_end:\n","                      ## the action is taken and the client is also picked up but the vehicle receives a penalty\n","                      selected_action, next_state = envs[v_id].take_action(envs[v_id].vehicle.current_travel_time, next_client_id)\n","                      reward = envs[v_id].reward_function_3(selected_action, True, False)\n","                      envs[v_id].clients[next_state.location_id].picked_up()\n","\n","                  next_state_vector = next_state.client_vector(next_client_id)\n","\n","              agents[v_id].store_memory(current_state_vector, actions, reward, np.array([next_state_vector]), int(envs[v_id].done))\n","\n","              vehicle_step_acumulative_reward[(episode, v_id)] += reward\n","\n","              envs[v_id].current_state = next_state\n","              envs[v_id].current_route.append(next_client_id)\n","\n","              ## updating the whole environment for all the vehicles to know what clients have been picked up by others\n","              for item in envs.keys():\n","                envs[item].current_state.clients_demands = envs[v_id].current_state.clients_demands\n","                envs[item].clients = envs[v_id].clients\n","\n","              if global_time_step % agent_update_frequency == 0 and global_time_step > 0:\n","                # print(\"Training_phase\")\n","                agents[v_id].learn()\n","\n","              envs[v_id].done = envs[v_id].terminal_check() ## if the terminal condition has been seen\n","              if envs[v_id].done:\n","                agents[v_id].update_exploration_rate()\n","\n","                if envs[v_id].current_route[-1] not in list(depots.keys()):\n","                  selected_action, next_state = envs[v_id].return_to_depot(envs[v_id].vehicle.current_travel_time)\n","                  reward = envs[v_id].reward_function_3(selected_action, False, True)\n","                  next_client_id = envs[v_id].vehicle.current_arrival_id\n","                  next_state_vector = depot_vectors(envs[v_id].depots, envs[v_id].vehicle, envs[v_id].vehicle.current_arrival_id)\n","                  action_id = envs[v_id].vehicle.current_arrival_id\n","                  # print(\"The process is done for the vehicle with id number = \", v_id, \"reward = \", reward, next_client_id)\n","                  vehicle_step_acumulative_reward[(episode, v_id)] += reward\n","\n","                  envs[v_id].current_route.append(next_client_id)\n","                  envs[v_id].routes.append(envs[v_id].current_route)\n","                  envs[v_id].current_route = []\n","                else:\n","                  envs[v_id].routes.append(envs[v_id].current_route)\n","                  envs[v_id].current_route = []\n","\n","                continue ## go on with the other vehicle\n","\n","              envs[v_id].vehicle.current_travel_time += 1\n","              # print(\"================ vehicle\", v_id,\"================\")\n","\n","        global_time_step += 1\n","      print(\"============ episode\", episode,\" finished ============\")\n","\n","      for v_index in envs.keys():\n","          score_history[v_index].append(vehicle_step_acumulative_reward[(episode,v_index)])\n","\n","      # for v_id in range(n_vehicles):\n","      #   mean_valid[(episode, v_id)], valid_routes[(episode, v_id)], agents[v_id] = validate_multi_vehicle(valid_df, n_vehicles ,v_id, agents[v_id])\n","      #   if mean_valid[(episode, v_id)] > best_rewards[v_id]:\n","      #     best_rewards[v_id] = mean_valid[(episode, v_id)]\n","\n","      for v_index in envs.keys():\n","        total_episode_route[(episode, v_index)] = envs[v_index].routes\n","        avg_score[v_index] = np.mean(score_history[v_index][-100:])\n","        if avg_score[v_index] > best_score[v_index]:\n","          best_score[v_index] = avg_score[v_index]\n","          # if not load_checkpoint:\n","          #   agents[v_index].save_models()\n","\n","  train_results = dict()\n","  train_results[\"routes\"] = total_episode_route\n","  train_results[\"ac_rewards\"] = vehicle_step_acumulative_reward\n","\n","  valid_results = dict()\n","  valid_results['mean_valid'] = mean_valid\n","  valid_results['valid_routes'] = valid_routes\n","  valid_results['best_rewards'] = best_rewards\n","\n","  return train_results, valid_results, agents"],"metadata":{"id":"b7lRD0CEjolc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main Function"],"metadata":{"id":"MUvUw_10oJRf"}},{"cell_type":"code","source":["import time\n","start_time = time.time()\n","\n","n_episodes = 500\n","n_time_steps = 1000\n","# n_vehicles = vehicle_num[100]\n","n_vehicles = 3\n","agent_update_frequency = 10\n","\n","train_results, valid_results, actors = train_multi_vehicle (n_episodes, n_time_steps,train_df, valid_df, n_vehicles, agent_update_frequency = 10)\n","\n","finish_time = time.time()"],"metadata":{"id":"ytlDHhtGoNH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"start time => {start_time}\")\n","print(f\"finsih time => {finish_time}\")\n","\n","print(f\"taken time for {n_episodes} episodes is {(finish_time-start_time)/60} mins\")"],"metadata":{"id":"ygyNxpPxjZ9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e5zPAETHLHPk"},"source":["# Evaluations 25 50 100"]},{"cell_type":"markdown","source":["### Function"],"metadata":{"id":"0GYy53ljlCnh"}},{"cell_type":"code","source":["import time"],"metadata":{"id":"l626CU7bOhmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluation_multi_vehicle(n_episodes, eval_df, networks, n_vehicles):\n","  eval_data = create_data_ind(eval_df, 1)\n","  eval_data = adding_depot_vehicles(eval_data, n_vehicles-1)\n","  clients = eval_data['clients']\n","  vehicles = eval_data['vehicles']\n","  depots = eval_data['depots']\n","\n","  state_dim = 6\n","  action_dim = (1,len(clients))\n","\n","  evaluate = False\n","  exploration_rate = 0.4\n","\n","  load_checkpoint = False\n","  best_score = dict()\n","  avg_score = dict()\n","  score_history = dict()\n","\n","  envs = dict() ## one environment for each vehicle\n","  total_vehicle_routes = dict() ## total passed routes for each vehicle based on each episode\n","  total_episode_route = dict() ## total passed routes for all the episodes (inside of it is the above dictionary)\n","  vehicle_step_acumulative_reward = dict() ## acumulative rewards for each vehicle at the end of each episode\n","  agents = dict()\n","  Target_nets = dict()\n","\n","  mean_valid = {}\n","  valid_routes = {}\n","  best_rewards = {}\n","\n","  for vehicle in vehicles.keys():\n","      agents[vehicle] = networks[vehicle]\n","      total_vehicle_routes[vehicle] = dict()\n","      best_score[vehicle] = -np.inf\n","      avg_score[vehicle] = 0\n","      score_history[vehicle] = []\n","      best_rewards[vehicle] = -np.inf\n","\n","  for episode in range(n_episodes):\n","\n","      # print(\"\\n episode = \", episode)\n","      global_time_step = 0\n","\n","      eval_data = create_data_ind(eval_df, 1)\n","      eval_data = adding_depot_vehicles(eval_data, n_vehicles-1)\n","      clients = eval_data['clients']\n","      vehicles = eval_data['vehicles']\n","      depots = eval_data['depots']\n","\n","      for vehicle_id in vehicles.keys():\n","          vehicle_step_acumulative_reward[(episode, vehicle_id)] = 0\n","          envs[vehicle_id] = VRP_environment(clients, vehicles[vehicle_id], vehicle_id, depots)\n","          envs[vehicle_id].routes = []\n","          envs[vehicle_id].current_route = [envs[vehicle_id].vehicle.current_departure_id]\n","          envs[vehicle_id].done = False\n","          envs[vehicle_id].full_capacity = False\n","          envs[vehicle_id].vehicle.current_travel_time = 0\n","\n","      while global_time_step <= n_time_steps:\n","        for v_id in range(n_vehicles):\n","          if global_time_step < envs[v_id].vehicle.current_travel_time and envs[v_id].idle:\n","                reward = -1000\n","                vehicle_step_acumulative_reward[(episode, v_id)] += reward\n","\n","          elif global_time_step >= envs[v_id].vehicle.current_travel_time and not envs[v_id].done:\n","              demands = set(envs[v_id].current_state.clients_demands.values())\n","              min_demand = 0\n","              if 0 in demands:\n","                demands.remove(0)\n","              if len(demands)>0:\n","                min_demand =  min(demands)\n","              if envs[v_id].full_capacity or envs[v_id].vehicle.capacity < min_demand: ##ToDO: recheck for the random or deliberate selection of the depot to go\n","                  selected_action, next_state = envs[v_id].return_to_depot(envs[v_id].vehicle.current_travel_time)\n","                  reward = envs[v_id].reward_function_3(selected_action, False, True)\n","                  next_client_id = envs[v_id].vehicle.current_arrival_id\n","                  next_state_vector = depot_vectors(envs[v_id].depots, envs[v_id].vehicle, envs[v_id].vehicle.current_arrival_id)\n","                  action_id = envs[v_id].vehicle.current_arrival_id\n","\n","                  envs[v_id].current_route.append(next_client_id)\n","                  envs[v_id].routes.append(envs[v_id].current_route)\n","                  envs[v_id].current_route = []\n","\n","              elif envs[v_id].remaining_state_check():\n","                  current_state_id = envs[v_id].current_state.location_id\n","                  if current_state_id in envs[v_id].depots.keys(): ## the current state is the depot\n","                      current_state_vector = depot_vectors(envs[v_id].depots, envs[v_id].vehicle, current_state_id)\n","                  elif current_state_id in envs[v_id].clients.keys(): ## the current state is one of the clients\n","                      current_state_vector = envs[v_id].current_state.client_vector(current_state_id)\n","                      current_state_vector = current_state_vector.reshape(1,6)\n","\n","                  envs[v_id].idle = False\n","\n","                  actions = agents[v_id].choose_action(current_state_vector, evaluate)\n","                  q_values = np.array(actions)\n","                  q_values = q_values.reshape((1, 100))[0]\n","                  q_values = q_values[0:len(envs[v_id].clients)]\n","                  next_client_id = np.argmax(q_values)\n","                  repeat = 0\n","                  while next_client_id not in list(envs[v_id].clients.keys()) and envs[v_id].remaining_state_check() and repeat <= len(envs[v_id].clients):\n","                      q_values[next_client_id] = -np.inf\n","                      next_client_id = np.argmax(q_values)\n","                      repeat += 1\n","                      if next_client_id in list(envs[v_id].clients.keys()) and not envs[v_id].clients[next_client_id].picked_up_flag:\n","                        break\n","\n","                  next_client = envs[v_id].clients[next_client_id]\n","\n","                  if envs[v_id].vehicle.capacity < next_client.demand or next_client.picked_up_flag == True:\n","                    continue\n","\n","                  if next_client.tw_start <= envs[v_id].vehicle.current_travel_time <= next_client.tw_end: ## checking the time window perservation constraint\n","                    selected_action, next_state = envs[v_id].take_action(envs[v_id].vehicle.current_travel_time, next_client_id)\n","                    reward = envs[v_id].reward_function_3(selected_action, False, False)\n","                    envs[v_id].clients[next_state.location_id].picked_up()\n","\n","                  elif envs[v_id].vehicle.current_travel_time < next_client.tw_start: ## the vehicle needs to wait for the time step in which the client will come\n","                      envs[v_id].idle = True\n","                      envs[v_id].vehicle.current_travel_time = next_client.tw_start ##shifting the time step to avoid wasting time\n","                      selected_action, next_state = envs[v_id].take_action(envs[v_id].vehicle.current_travel_time, next_client_id)\n","                      reward = envs[v_id].reward_function_3(selected_action, True, False)\n","                      envs[v_id].clients[next_state.location_id].picked_up()\n","\n","                  elif envs[v_id].vehicle.current_travel_time > next_client.tw_end:\n","                      ## the action is taken and the client is also picked up but the vehicle receives a penalty\n","                      selected_action, next_state = envs[v_id].take_action(envs[v_id].vehicle.current_travel_time, next_client_id)\n","                      reward = envs[v_id].reward_function_3(selected_action, True, False)\n","                      envs[v_id].clients[next_state.location_id].picked_up()\n","\n","                  next_state_vector = next_state.client_vector(next_client_id)\n","                  agents[v_id].store_memory(current_state_vector, actions, reward, np.array([next_state_vector]), int(envs[v_id].done))\n","\n","              vehicle_step_acumulative_reward[(episode, v_id)] += reward\n","\n","              envs[v_id].current_state = next_state\n","              envs[v_id].current_route.append(next_client_id)\n","\n","              ## updating the whole environment for all the vehicles to know what clients have been picked up by others\n","              for item in envs.keys():\n","                envs[item].current_state.clients_demands = envs[v_id].current_state.clients_demands\n","                envs[item].clients = envs[v_id].clients\n","\n","              envs[v_id].done = envs[v_id].terminal_check() ## if the terminal condition has been seen\n","              if envs[v_id].done:\n","                agents[v_id].update_exploration_rate()\n","\n","                if envs[v_id].current_route[-1] not in list(depots.keys()):\n","                  selected_action, next_state = envs[v_id].return_to_depot(envs[v_id].vehicle.current_travel_time)\n","                  reward = envs[v_id].reward_function_3(selected_action, False, True)\n","                  next_client_id = envs[v_id].vehicle.current_arrival_id\n","                  next_state_vector = depot_vectors(envs[v_id].depots, envs[v_id].vehicle, envs[v_id].vehicle.current_arrival_id)\n","                  action_id = envs[v_id].vehicle.current_arrival_id\n","                  # print(\"The process is done for the vehicle with id number = \", v_id, \"reward = \", reward, next_client_id)\n","                  vehicle_step_acumulative_reward[(episode, v_id)] += reward\n","\n","                  envs[v_id].current_route.append(next_client_id)\n","                  envs[v_id].routes.append(envs[v_id].current_route)\n","                  envs[v_id].current_route = []\n","                else:\n","                  envs[v_id].routes.append(envs[v_id].current_route)\n","                  envs[v_id].current_route = []\n","\n","                continue ## go on with the other vehicle\n","\n","              envs[v_id].vehicle.current_travel_time += 1\n","              # print(\"================ vehicle\", v_id,\"================\")\n","\n","        global_time_step += 1\n","      # print(\"============ episode\", episode,\" finished ============\")\n","\n","      for v_index in envs.keys():\n","          score_history[v_index].append(vehicle_step_acumulative_reward[(episode,v_index)])\n","\n","      for v_index in envs.keys():\n","        total_episode_route[(episode, v_index)] = envs[v_index].routes\n","        avg_score[v_index] = np.mean(score_history[v_index][-100:])\n","        if avg_score[v_index] > best_score[v_index]:\n","          best_score[v_index] = avg_score[v_index]\n","\n","  eval_results = dict()\n","  eval_results[\"routes\"] = total_episode_route\n","  eval_results[\"ac_rewards\"] = vehicle_step_acumulative_reward\n","\n","  return eval_results, agents"],"metadata":{"id":"6qvObUsibPH9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data"],"metadata":{"id":"QpFybCdJlF9Z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UM6utkr3LHPl"},"outputs":[],"source":["eval_sample = 1\n","data_df_test = pd.read_csv(data_path, delim_whitespace=True)\n","evaluation_df_1 = evaluation_df_2 = evaluation_df_3 = evaluation_df_4 = data_df_test[0:depot_num]\n","evaluation_df_1 = pd.concat([evaluation_df_1, data_df_test[1:16]], ignore_index=True)\n","evaluation_df_2 = pd.concat([evaluation_df_2, data_df_test[1:25]], ignore_index=True)\n","evaluation_df_3 = pd.concat([evaluation_df_3, data_df_test[1:50]], ignore_index=True)\n","evaluation_df_4 = pd.concat([evaluation_df_4, data_df_test[1:100]], ignore_index=True)"]},{"cell_type":"markdown","source":["## 16"],"metadata":{"id":"reZm5Yi2yNzx"}},{"cell_type":"code","source":["## evaluation 1\n","## 16 customers\n","evaluation_data = create_data_ind(evaluation_df_1, depot_num)\n","evaluation_data = adding_depot_vehicles(evaluation_data, n_vehicles-1)\n","\n","v_num = 3\n","test_mean_reward = []\n","eval_tour_indices = []\n","# eval_tour_indices = dict()\n","# times = []\n","# for rep_sam in range(10):\n","#   print(rep_sam)\n","#   start_time = time.time()\n","#   eval_r, _ = evaluation_multi_vehicle(eval_sample, evaluation_df_1, actors, v_num)\n","#   finish_time = time.time()\n","#   eval_tour_indices[rep_sam] = eval_r[\"routes\"]\n","#   print(\"Time => \", (finish_time-start_time))\n","#   times.append(float(finish_time-start_time))\n","# print(\"Solution time \", min(times))\n","# print(eval_tour_indices)\n","\n","times = []\n","\n","start_time = time.time()\n","eval_r, _ = evaluation_multi_vehicle(eval_sample, evaluation_df_1, actors, v_num)\n","finish_time = time.time()\n","eval_tour_indices = eval_r[\"routes\"]\n","print(\"Time => \", (finish_time-start_time))\n","times.append(float(finish_time-start_time))\n","\n","from scipy.spatial.distance import euclidean\n","depot_id = list(evaluation_data['depots'].keys())\n","# print(\"depot_id\",depot_id)\n","print(\"Dataset =>\", dataset, dataset_num)\n","\n","evaluation_distances = []\n","for key in eval_tour_indices.keys():\n","  dist = 0\n","  r = eval_tour_indices[key]\n","  print(key, r)\n","  for route in r:\n","    if len(route)<=2:\n","      continue\n","    for i in range(0, len(route)-1):\n","      if route[i] in evaluation_data['depots'].keys():\n","        # continue\n","        a = (evaluation_data['depots'][route[i]]['dep_x'], evaluation_data['depots'][route[i]]['dep_y'])\n","        b = (evaluation_data['clients'][route[i+1]].cx, evaluation_data['clients'][route[i+1]].cy)\n","      elif route[i+1] in evaluation_data['depots'].keys():\n","        # continue\n","        a = (evaluation_data['clients'][route[i]].cx, evaluation_data['clients'][route[i]].cy)\n","        b = (evaluation_data['depots'][route[i+1]]['dep_x'], evaluation_data['depots'][route[i+1]]['dep_y'])\n","      else:\n","        a = (evaluation_data['clients'][route[i]].cx, evaluation_data['clients'][route[i]].cy)\n","        b = (evaluation_data['clients'][route[i+1]].cx, evaluation_data['clients'][route[i+1]].cy)\n","\n","      # print(route[i], route[i+1], a, b)\n","      euclidean_distance = euclidean(a,b)\n","      dist += euclidean_distance\n","  print(dist)\n","  evaluation_distances.append(dist)\n","\n","print(\"Solution time \", min(times))\n","print(evaluation_distances)\n","print(\"3 vehicle\")\n","print(f\"Mean distance in evaluation 16 data {np.mean(evaluation_distances)}\")\n","print(f\"Sum distance in evaluation 16 data {np.sum(evaluation_distances)}\")\n","print(\"Route => \", eval_tour_indices)\n","print(\"\\n================ evaluation 1 finished ======================\\n\")\n"],"metadata":{"id":"mvXqsUn-yPJk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 25"],"metadata":{"id":"uM6mg0REe7P3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjV3JBVuLHPl"},"outputs":[],"source":["## evaluation 2\n","## 25 customers\n","evaluation_data = create_data_ind(evaluation_df_2, depot_num)\n","evaluation_data = adding_depot_vehicles(evaluation_data, n_vehicles-1)\n","\n","# v_num = vehicle_num[25]\n","v_num = 3\n","test_mean_reward = []\n","eval_tour_indices = dict()\n","\n","times = []\n","for rep_sam in range(10):\n","  print(rep_sam)\n","  start_time = time.time()\n","  eval_r, _ = evaluation_multi_vehicle(eval_sample, evaluation_df_2, actors, v_num)\n","  finish_time = time.time()\n","  eval_tour_indices[rep_sam] = eval_r[\"routes\"]\n","  print(\"Time => \", (finish_time-start_time))\n","  times.append(float(finish_time-start_time))\n","\n","print(\"Solution time \", min(times))\n","print(eval_tour_indices)\n","\n","eval_tour_indices = []\n","start_time = time.time()\n","eval_r, _ = evaluation_multi_vehicle(eval_sample, evaluation_df_2, actors, v_num)\n","eval_tour_indices = eval_r[\"routes\"]\n","print(\"Time taken => \", (time.time()-start_time))\n","\n","from scipy.spatial.distance import euclidean\n","depot_id = list(evaluation_data['depots'].keys())\n","print(\"depot_id\",depot_id)\n","\n","evaluation_distances = []\n","for key in eval_tour_indices.keys():\n","  dist = 0\n","  r = eval_tour_indices[key]\n","  print(key, r)\n","  for route in r:\n","    if len(route)<=2:\n","      continue\n","    for i in range(0, len(route)-1):\n","      if route[i] in evaluation_data['depots'].keys():\n","        # continue\n","        a = (evaluation_data['depots'][route[i]]['dep_x'], evaluation_data['depots'][route[i]]['dep_y'])\n","        b = (evaluation_data['clients'][route[i+1]].cx, evaluation_data['clients'][route[i+1]].cy)\n","      elif route[i+1] in evaluation_data['depots'].keys():\n","        # continue\n","        a = (evaluation_data['clients'][route[i]].cx, evaluation_data['clients'][route[i]].cy)\n","        b = (evaluation_data['depots'][route[i+1]]['dep_x'], evaluation_data['depots'][route[i+1]]['dep_y'])\n","      else:\n","        a = (evaluation_data['clients'][route[i]].cx, evaluation_data['clients'][route[i]].cy)\n","        b = (evaluation_data['clients'][route[i+1]].cx, evaluation_data['clients'][route[i+1]].cy)\n","\n","      # print(route[i], route[i+1], a, b)\n","      euclidean_distance = euclidean(a,b)\n","      dist += euclidean_distance\n","  print(dist)\n","  evaluation_distances.append(dist)\n","\n","# print(\"results for vehicles =>\", vehicle_num[25])\n","print(evaluation_distances)\n","print(f\"Mean distance in evaluation 25 data {np.mean(evaluation_distances)}\")\n","print(f\"Sum distance in evaluation 50 data {np.sum(evaluation_distances)}\")\n","print(\"\\n================ evaluation 2 finished ======================\\n\")"]},{"cell_type":"markdown","metadata":{"id":"l0YoHfS3LHPm"},"source":["## 50"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lfdjQ-ALHPm"},"outputs":[],"source":["## evaluation 2\n","## 50 customers\n","evaluation_data = create_data_ind(evaluation_df_3, depot_num)\n","evaluation_data = adding_depot_vehicles(evaluation_data, n_vehicles-1)\n","\n","# v_num = vehicle_num[50]\n","v_num = 1\n","test_mean_reward = []\n","\n","eval_tour_indices = dict()\n","\n","# times = []\n","# for rep_sam in range(10):\n","#   print(rep_sam)\n","#   start_time = time.time()\n","#   eval_r, _ = evaluation_multi_vehicle(eval_sample, evaluation_df_3, actors, v_num)\n","#   finish_time = time.time()\n","#   eval_tour_indices[rep_sam] = eval_r[\"routes\"]\n","#   print(\"Time => \", (finish_time-start_time))\n","#   times.append(float(finish_time-start_time))\n","\n","# print(\"Solution time \", min(times))\n","# print(eval_tour_indices)\n","\n","# eval_tour_indices = []\n","\n","start_time = time.time()\n","eval_r, _ = evaluation_multi_vehicle(eval_sample, evaluation_df_3, actors, v_num)\n","eval_tour_indices = eval_r[\"routes\"]\n","print(\"Time taken => \", (time.time()-start_time))\n","\n","from scipy.spatial.distance import euclidean\n","depot_id = list(evaluation_data['depots'].keys())\n","print(\"depot_id\",depot_id)\n","\n","evaluation_distances = []\n","for key in eval_tour_indices.keys():\n","  dist = 0\n","  r = eval_tour_indices[key]\n","  print(key, r)\n","  for route in r:\n","    if len(route)<=2:\n","      continue\n","    for i in range(0, len(route)-1):\n","      if route[i] in evaluation_data['depots'].keys():\n","        # continue\n","        a = (evaluation_data['depots'][route[i]]['dep_x'], evaluation_data['depots'][route[i]]['dep_y'])\n","        b = (evaluation_data['clients'][route[i+1]].cx, evaluation_data['clients'][route[i+1]].cy)\n","      elif route[i+1] in evaluation_data['depots'].keys():\n","        # continue\n","        a = (evaluation_data['clients'][route[i]].cx, evaluation_data['clients'][route[i]].cy)\n","        b = (evaluation_data['depots'][route[i+1]]['dep_x'], evaluation_data['depots'][route[i+1]]['dep_y'])\n","      else:\n","        a = (evaluation_data['clients'][route[i]].cx, evaluation_data['clients'][route[i]].cy)\n","        b = (evaluation_data['clients'][route[i+1]].cx, evaluation_data['clients'][route[i+1]].cy)\n","      euclidean_distance = euclidean(a,b)\n","      dist += euclidean_distance\n","  print(dist)\n","  evaluation_distances.append(dist)\n","\n","# print(\"results for vehicles =>\", vehicle_num[50])\n","print(evaluation_distances)\n","print(f\"Mean distance in evaluation 50 data {np.mean(evaluation_distances)}\")\n","print(f\"Sum distance in evaluation 50 data {np.sum(evaluation_distances)}\")\n","\n","print(\"\\n================ evaluation 3 finished ======================\\n\")"]},{"cell_type":"markdown","metadata":{"id":"DwD1Z1itLHPm"},"source":["## 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_453Tp_LHPm"},"outputs":[],"source":["## evaluation 2\n","## 25 customers\n","evaluation_data = create_data_ind(evaluation_df_4, depot_num)\n","evaluation_data = adding_depot_vehicles(evaluation_data, n_vehicles-1)\n","\n","# v_num = vehicle_num[100]\n","v_num= 1\n","test_mean_reward = []\n","\n","eval_tour_indices = dict()\n","\n","# times = []\n","# for rep_sam in range(10):\n","#   print(rep_sam)\n","#   start_time = time.time()\n","#   eval_r, _ = evaluation_multi_vehicle(eval_sample, evaluation_df_4, actors, v_num)\n","#   finish_time = time.time()\n","#   eval_tour_indices[rep_sam] = eval_r[\"routes\"]\n","#   print(\"Time => \", (finish_time-start_time))\n","#   times.append(float(finish_time-start_time))\n","\n","\n","# print(\"Solution time \", min(times))\n","# print(eval_tour_indices)\n","\n","# # eval_tour_indices = []\n","\n","start_time = time.time()\n","eval_r, _ = evaluation_multi_vehicle(eval_sample, evaluation_df_3, actors, v_num)\n","eval_tour_indices = eval_r[\"routes\"]\n","print(\"Time taken => \", (time.time()-start_time))\n","\n","from scipy.spatial.distance import euclidean\n","depot_id = list(evaluation_data['depots'].keys())\n","print(\"depot_id\",depot_id)\n","\n","evaluation_distances = []\n","for key in eval_tour_indices.keys():\n","  dist = 0\n","  r = eval_tour_indices[key]\n","  print(key, r)\n","  for route in r:\n","    if len(route)<=2:\n","      continue\n","    for i in range(0, len(route)-1):\n","      if route[i] in evaluation_data['depots'].keys():\n","        # continue\n","        a = (evaluation_data['depots'][route[i]]['dep_x'], evaluation_data['depots'][route[i]]['dep_y'])\n","        b = (evaluation_data['clients'][route[i+1]].cx, evaluation_data['clients'][route[i+1]].cy)\n","      elif route[i+1] in evaluation_data['depots'].keys():\n","        # continue\n","        a = (evaluation_data['clients'][route[i]].cx, evaluation_data['clients'][route[i]].cy)\n","        b = (evaluation_data['depots'][route[i+1]]['dep_x'], evaluation_data['depots'][route[i+1]]['dep_y'])\n","      else:\n","        a = (evaluation_data['clients'][route[i]].cx, evaluation_data['clients'][route[i]].cy)\n","        b = (evaluation_data['clients'][route[i+1]].cx, evaluation_data['clients'][route[i+1]].cy)\n","      euclidean_distance = euclidean(a,b)\n","      dist += euclidean_distance\n","  print(dist)\n","  evaluation_distances.append(dist)\n","\n","# print(\"results for vehicles =>\", vehicle_num[100])\n","print(evaluation_distances)\n","print(f\"Mean distance in evaluation 100 data {np.mean(evaluation_distances)}\")\n","print(f\"Sum distance in evaluation 50 data {np.sum(evaluation_distances)}\")\n","print(\"\\n================ evaluation 4 finished ======================\\n\")"]}]}